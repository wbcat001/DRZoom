{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d85ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# âš ï¸ é‡è¦: ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ã€ã‚ãªãŸã®ç’°å¢ƒã®WordNetãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆdictãƒ•ã‚©ãƒ«ãƒ€å†…ãªã©ï¼‰ã®æ­£ã—ã„ãƒ‘ã‚¹ã«ã™ã¹ã¦ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚\n",
    "WORDNET_FILE_PATHS = {\n",
    "    'noun': 'wordnet/dict/index.noun', \n",
    "    'verb': 'wordnet/dict/index.verb',\n",
    "    'adj': 'wordnet/dict/index.adj',\n",
    "    'adv': 'wordnet/dict/index.adv'\n",
    "}\n",
    "\n",
    "# WordNetã®ç”Ÿã®indexãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿è¡Œã®å‰ã«è‘—ä½œæ¨©æƒ…å ±ãªã©ã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒã‚ã‚Šã¾ã™ã€‚\n",
    "# WordNet 3.0ã®ä¸€èˆ¬çš„ãªãƒ˜ãƒƒãƒ€ãƒ¼è¡Œæ•°ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹è¡Œæ•°ï¼‰ã‚’è¨­å®šã—ã¾ã™ã€‚\n",
    "HEADER_LINES_TO_SKIP = 29 # WordNet 3.0ã®å ´åˆã®ç›®å®‰\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45bb38a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Indexãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...\n",
      "\n",
      "--- NOUNãƒ•ã‚¡ã‚¤ãƒ« (index.noun) ã‚’å‡¦ç†ä¸­ ---\n",
      "-> ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªæ•° (ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”±æ¥): 56955\n",
      "\n",
      "--- VERBãƒ•ã‚¡ã‚¤ãƒ« (index.verb) ã‚’å‡¦ç†ä¸­ ---\n",
      "-> ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªæ•° (ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”±æ¥): 8691\n",
      "\n",
      "--- ADJãƒ•ã‚¡ã‚¤ãƒ« (index.adj) ã‚’å‡¦ç†ä¸­ ---\n",
      "-> ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªæ•° (ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”±æ¥): 20718\n",
      "\n",
      "--- ADVãƒ•ã‚¡ã‚¤ãƒ« (index.adv) ã‚’å‡¦ç†ä¸­ ---\n",
      "-> ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªæ•° (ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”±æ¥): 3736\n",
      "\n",
      "==================================================\n",
      "ğŸ“š å‡¦ç†çµæœã®æ¦‚è¦\n",
      "==================================================\n",
      "| å‡¦ç†å‰ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªç”Ÿå˜èªç·æ•°: 147,478 å€‹\n",
      "| ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªä¸»è¦å˜èªç·æ•°: 82,368 å€‹\n",
      "--------------------------------------------------\n",
      "| æœ€çµ‚çš„ã«é™¤å»ã•ã‚ŒãŸãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªç·æ•°: 65,110 å€‹\n",
      "--------------------------------------------------\n",
      "\n",
      "--- é™¤å»ç†ç”±ã®å†…è¨³ï¼ˆé‡è¤‡ã‚«ã‚¦ãƒ³ãƒˆã‚ã‚Šï¼‰---\n",
      "| è¤‡åˆèª (ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ '_')        : 64,375 å€‹\n",
      "| å›ºæœ‰åè©/å¤§æ–‡å­—ã‚’å«ã‚€              : 178 å€‹\n",
      "| è¨˜å·ã‚„æ•°å­—ã‚’å«ã‚€                 : 456 å€‹\n",
      "| çŸ­ã™ãã‚‹å˜èª (3æ–‡å­—æœªæº€)           : 358 å€‹\n",
      "==================================================\n",
      "\n",
      "ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªã®ä¾‹ (æœ€åˆã®20å€‹):\n",
      "1. a-bomb\n",
      "2. a-horizon\n",
      "3. a-line\n",
      "4. a-list\n",
      "5. a-ok\n",
      "6. a-okay\n",
      "7. a-one\n",
      "8. a-team\n",
      "9. aaa\n",
      "10. aachen\n",
      "11. aah\n",
      "12. aalborg\n",
      "13. aalii\n",
      "14. aalst\n",
      "15. aalto\n",
      "16. aar\n",
      "17. aardvark\n",
      "18. aardwolf\n",
      "19. aare\n",
      "20. aarhus\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¢æ•° ---\n",
    "\n",
    "def clean_and_filter_lemma(lemma, exclusion_counts, raw_lemmas_set):\n",
    "    \"\"\"\n",
    "    å˜èªã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ä¸»è¦ãªå˜èªã¨ã—ã¦é©åˆ‡ã‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã€‚\n",
    "    \n",
    "    Args:\n",
    "        lemma (str): WordNetã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸç”Ÿã®å˜èªï¼ˆãƒ¬ãƒï¼‰ã€‚\n",
    "        exclusion_counts (defaultdict): é™¤å¤–ç†ç”±ã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã®è¾æ›¸ã€‚\n",
    "        raw_lemmas_set (set): ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰ã®å…¨ã¦ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªã‚’è¨˜éŒ²ã™ã‚‹é›†åˆã€‚\n",
    "        \n",
    "    Returns:\n",
    "        str or None: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é€šéã—ãŸå˜èªï¼ˆå°æ–‡å­—åŒ–ï¼‰ã¾ãŸã¯ Noneã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    # å‡¦ç†å‰ã®é›†åˆã«ç”Ÿã®ãƒ¬ãƒï¼ˆWordNetå½¢å¼ï¼‰ã‚’è¿½åŠ \n",
    "    # raw_lemmas_set.add(lemma) ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã®é‡è¤‡ã‚’é™¤å¤–ã™ã‚‹ãŸã‚ã€\n",
    "    # å‘¼ã³å‡ºã—å…ƒï¼ˆextract_lemmas_with_filteringï¼‰ã§è¡Œã†ã®ãŒé©åˆ‡ã§ã™ã€‚\n",
    "    \n",
    "    # 1. è¤‡åˆèªï¼ˆã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ï¼‰ã®é™¤å¤–\n",
    "    if '_' in lemma:\n",
    "        exclusion_counts['Compound_Word'] += 1\n",
    "        return None\n",
    "        \n",
    "    # 2. å›ºæœ‰åè©/å¤§æ–‡å­—ã®é™¤å¤– (å…¨ã¦å°æ–‡å­—ã§ã‚ã‚‹ã“ã¨)\n",
    "    if not lemma.islower():\n",
    "        exclusion_counts['Proper_Noun_or_Capitalized'] += 1\n",
    "        return None\n",
    "        \n",
    "    cleaned_lemma = lemma.lower()\n",
    "    \n",
    "    # 3. æ•°å­—ãƒ»è¨˜å·ã®é™¤å¤– (ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¨ãƒã‚¤ãƒ•ãƒ³ã®ã¿ã‚’è¨±å¯)\n",
    "    if re.search(r'[^a-z-]', cleaned_lemma):\n",
    "        exclusion_counts['Symbol_or_Numeric'] += 1\n",
    "        return None\n",
    "        \n",
    "    # 4. çŸ­ã™ãã‚‹å˜èªã®é™¤å¤– (é•·ã•ãŒ3æ–‡å­—æœªæº€ã®ã‚‚ã®ã‚’é™¤å»)\n",
    "    min_word_length = 3\n",
    "    if len(cleaned_lemma) < min_word_length:\n",
    "        exclusion_counts['Too_Short'] += 1\n",
    "        return None\n",
    "        \n",
    "    return cleaned_lemma\n",
    "\n",
    "# --- æŠ½å‡ºé–¢æ•°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨è¨ˆä¸Šã‚’é©ç”¨ï¼‰ ---\n",
    "\n",
    "def extract_lemmas_with_filtering(file_path, exclusion_counts, raw_lemmas_set):\n",
    "    \"\"\"\n",
    "    indexãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å˜èªã‚’æŠ½å‡ºã—ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã†ã€‚\n",
    "    ç”Ÿã®å˜èªã‚’ raw_lemmas_set ã«è¿½åŠ ã—ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®å˜èªã‚’è¿”ã™ã€‚\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"[è­¦å‘Š] ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
    "        return set()\n",
    "\n",
    "    filtered_lemmas = set()\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for _ in range(HEADER_LINES_TO_SKIP):\n",
    "                next(f, None)\n",
    "                \n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith(' '):\n",
    "                    continue\n",
    "                    \n",
    "                parts = line.split()\n",
    "                if parts:\n",
    "                    raw_lemma = parts[0]\n",
    "                    \n",
    "                    # ğŸ’¡ ç”Ÿã®å˜èªã‚’å³åº§ã«é›†åˆã«è¿½åŠ ï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰\n",
    "                    raw_lemmas_set.add(raw_lemma)\n",
    "                    \n",
    "                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¢æ•°ã‚’é©ç”¨\n",
    "                    cleaned_lemma = clean_and_filter_lemma(raw_lemma, exclusion_counts, raw_lemmas_set)\n",
    "                    \n",
    "                    if cleaned_lemma:\n",
    "                        filtered_lemmas.add(cleaned_lemma)\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"[ã‚¨ãƒ©ãƒ¼] ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "        return set()\n",
    "\n",
    "    return filtered_lemmas\n",
    "\n",
    "# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
    "raw_lemmas_set = set() # ğŸ‘ˆ å‡¦ç†å‰ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªé›†åˆï¼ˆWordNetå½¢å¼ï¼‰\n",
    "all_unique_lemmas = set() # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªé›†åˆ\n",
    "exclusion_counts = defaultdict(int)\n",
    "processed_files = 0\n",
    "\n",
    "print(\"WordNet Indexãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "\n",
    "for pos, path in WORDNET_FILE_PATHS.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"\\n--- {pos.upper()}ãƒ•ã‚¡ã‚¤ãƒ« ({os.path.basename(path)}) ã‚’å‡¦ç†ä¸­ ---\")\n",
    "        \n",
    "        # raw_lemmas_set ã«ã¯ç”Ÿã®å˜èªãŒè¿½åŠ ã•ã‚Œã€filtered_lemmas ã«ã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®å˜èªãŒè¿½åŠ ã•ã‚Œã‚‹\n",
    "        filtered_lemmas = extract_lemmas_with_filtering(path, exclusion_counts, raw_lemmas_set)\n",
    "        \n",
    "        all_unique_lemmas.update(filtered_lemmas)\n",
    "        processed_files += 1\n",
    "        print(f\"-> ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªæ•° (ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”±æ¥): {len(filtered_lemmas)}\")\n",
    "\n",
    "# --- çµæœã®è¡¨ç¤º ---\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“š å‡¦ç†çµæœã®æ¦‚è¦\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# å‡¦ç†å‰ã®é›†åˆã‚µã‚¤ã‚ºï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªç”Ÿå˜èªæ•°ï¼‰\n",
    "total_raw_unique_lemmas = len(raw_lemmas_set)\n",
    "# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®é›†åˆã‚µã‚¤ã‚º\n",
    "total_filtered_unique_lemmas = len(all_unique_lemmas)\n",
    "\n",
    "# é™¤å»ã•ã‚ŒãŸå˜èªæ•°ã¯ã€ç”Ÿã®é›†åˆã‹ã‚‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®é›†åˆã‚’å¼•ã„ã¦ç®—å‡ºã—ã¾ã™ã€‚\n",
    "# é™¤å»ã•ã‚ŒãŸå˜èªã®ç·æ•° = ç”Ÿã®ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•° - ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°\n",
    "total_removed = total_raw_unique_lemmas - total_filtered_unique_lemmas\n",
    "\n",
    "\n",
    "print(f\"| å‡¦ç†å‰ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªç”Ÿå˜èªç·æ•°: {total_raw_unique_lemmas:,} å€‹\")\n",
    "print(f\"| ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªä¸»è¦å˜èªç·æ•°: {total_filtered_unique_lemmas:,} å€‹\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"| æœ€çµ‚çš„ã«é™¤å»ã•ã‚ŒãŸãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªç·æ•°: {total_removed:,} å€‹\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n--- é™¤å»ç†ç”±ã®å†…è¨³ï¼ˆé‡è¤‡ã‚«ã‚¦ãƒ³ãƒˆã‚ã‚Šï¼‰---\")\n",
    "# é™¤å»ç†ç”±ã¨ãã®å˜èªæ•°ã®ãƒªã‚¹ãƒˆ\n",
    "removal_reasons = {\n",
    "    \"Compound_Word\": \"è¤‡åˆèª (ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ '_')\",\n",
    "    \"Proper_Noun_or_Capitalized\": \"å›ºæœ‰åè©/å¤§æ–‡å­—ã‚’å«ã‚€\",\n",
    "    \"Symbol_or_Numeric\": \"è¨˜å·ã‚„æ•°å­—ã‚’å«ã‚€\",\n",
    "    \"Too_Short\": \"çŸ­ã™ãã‚‹å˜èª (3æ–‡å­—æœªæº€)\"\n",
    "}\n",
    "\n",
    "# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¢æ•°å†…ã§ã‚«ã‚¦ãƒ³ãƒˆã•ã‚ŒãŸæ•°ã‚’è¡¨ç¤º\n",
    "for key, description in removal_reasons.items():\n",
    "    count = exclusion_counts[key]\n",
    "    print(f\"| {description:<25}: {count:,} å€‹\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®å˜èªã®ä¾‹ã‚’è¡¨ç¤º\n",
    "unique_word_list = sorted(list(all_unique_lemmas))\n",
    "print(\"\\nãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªã®ä¾‹ (æœ€åˆã®20å€‹):\")\n",
    "for i, word in enumerate(unique_word_list[:20]):\n",
    "    print(f\"{i+1}. {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c559f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of w2v_words: 2999996\n",
      "Number of common words between raw WordNet lemmas and Word2Vec vocabulary: 54623\n",
      "Number of common words between WordNet filtered lemmas and Word2Vec vocabulary: 48272\n"
     ]
    }
   ],
   "source": [
    "# word2vec(googlenews-vectors-negative300.bin)ã®å˜èªãƒªã‚¹ãƒˆã¨ã®ä¸€è‡´ã‚’ç¢ºèªã™ã‚‹\n",
    "\n",
    "file_path = \"../22_word_label/processed_data/w2v_words_full.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    w2v_words = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "print(f\"length of w2v_words: {len(w2v_words)}\") # 3000000\n",
    "\n",
    "# common: å‡¦ç†å‰ã®WordNet\n",
    "common_words = raw_lemmas_set.intersection(w2v_words)\n",
    "print(f\"Number of common words between raw WordNet lemmas and Word2Vec vocabulary: {len(common_words)}\")\n",
    "# common: å‡¦ç†å¾Œã®WordNet\n",
    "common_words = all_unique_lemmas.intersection(w2v_words)\n",
    "print(f\"Number of common words between WordNet filtered lemmas and Word2Vec vocabulary: {len(common_words)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82892e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è¤‡åˆèªã®ä¾‹:\n",
      "golden_state\n",
      "soccer_field\n",
      "eugene_ionesco\n",
      "ischemic_stroke\n",
      "bull_pine\n",
      "finnan_haddock\n",
      "scrutin_uninominal_voting_system\n",
      "podocarpus_amara\n",
      "grimm's_law\n",
      "cichorium_endivia\n",
      "pumping_station\n",
      "genus_planera\n",
      "news_leak\n",
      "corn_spurrey\n",
      "air_combat_command\n",
      "ho_chi_minh_city\n",
      "bok_choy\n",
      "lacewing_fly\n",
      "shiny_lyonia\n",
      "survivorship_annuity\n"
     ]
    }
   ],
   "source": [
    "# è¤‡åˆèªã®ä¾‹(wordnet)ã‚’è¡¨ç¤º\n",
    "print(\"\\nè¤‡åˆèªã®ä¾‹:\")\n",
    "compound_examples = [lemma for lemma in raw_lemmas_set if '_' in lemma]\n",
    "for example in compound_examples[:20]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93555366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vecã®è¤‡åˆèªã®ä¾‹:\n",
      "Dimetapp_Pediacare_Robitussin_Triaminic\n",
      "Sounding_gong\n",
      "Jon_Sioredas\n",
      "By_Ruth_Mosalski\n",
      "patented_windmill_windup\n",
      "Philip_Quast\n",
      "Understanding_Malaysia_Ikim\n",
      "CPSC_Posts\n",
      "Earnings_Preview_BJ_Wholesale\n",
      "ankle_brachial\n",
      "expedition_outfitter\n",
      "Escorts_Balanced_Fund\n",
      "Amy_Shailene_Woodley\n",
      "Rosanna_Capolingua\n",
      "fecal_occult\n",
      "Chung_Shing\n",
      "Once_Upon_Mattress\n",
      "Miraculins_Announces\n",
      "Aldon_Smith_DE\n",
      "Burdale_Financial\n",
      "Shelly_Gobin\n",
      "SUTENT_sunitinib_malate\n",
      "PureSpectrum_dimmable_CFL\n",
      "Natural_Gas_CHNG\n",
      "toronto_canada\n",
      "David_Engelthaler_epidemiologist\n",
      "AMEX_TIV\n",
      "Mohammed_Yousri\n",
      "By_Barney_Lerten\n",
      "Inc._NASDAQ_CEBK\n",
      "Ashoura_celebrations\n",
      "HOME_COMFORT\n",
      "NOAA_Geostationary_Operational\n",
      "nick_Los_Cabos\n",
      "Point_Defiance\n",
      "Xun_Zhou\n",
      "Ñ–n_Ğ°Æ–Æ–\n",
      "drought_tolerant_crops\n",
      "Zahid_Gulfam\n",
      "lateral_agility\n",
      "CHARLOTTE_CHAMBERS\n",
      "Tomi_Mine\n",
      "wholly_owned_subsidiary_ZenZuu\n",
      "Sheriff_Lonny_Pulkrabek\n",
      "Ifakara_Health_Institute\n",
      "Bermuda_shorts\n",
      "Beaverton_Tigard_Tualatin\n",
      "Attorney_Kurt_Alme\n",
      "Mahindra_Rajapakse\n",
      "WEAU_TV\n",
      "Chunxiao_fields\n",
      "lateral_ligaments\n",
      "Cardiolite_Â®_Kit\n",
      "Saparmurad_Niyazov\n",
      "TheSUBWAY.com_Announces\n",
      "BY_MARY\n",
      "Switzerland_Serono_SA\n",
      "Grinnell_Glacier\n",
      "#.#GHz_cordless\n",
      "assistant_Ino_Guerrero\n",
      "wing_Erik_Condra\n",
      "Lutheran_Church\n",
      "Mount_Olive_Miss.\n",
      "MICHELLE_Wie\n",
      "H._Lurie\n",
      "rhino_horns_weighing\n",
      "Banten_Province\n",
      "MIGHT_NOT_BE\n",
      "epidemiological_cohort\n",
      "Zidlicky_Min\n",
      "Estranged_husband\n",
      "Govinda_Prasad\n",
      "harshly_rebuked\n",
      "donkey_carts_piled\n",
      "#.####_Stg_dlr\n",
      "Joseph_Jaskiewicz\n",
      "By_Lisa_Yensen\n",
      "Belarus_Paulauskas\n",
      "TITLE_SPONSOR\n",
      "Cayenne_GTS_Porsche\n",
      "Oce_visit_www.oceusa.com\n",
      "H._Shonna_Yin\n",
      "Joan_Lascorz_Kawasaki_Motocard.com\n",
      "INC._AND_SUBSIDIARIES_Condensed\n",
      "Zwerling_Schachter_&\n",
      "Bob_Kanjian\n",
      "Sean_O'Casey_Juno\n",
      "Upper_Makefield\n",
      "PFG_Best\n",
      "Seuss_rhymes\n",
      "dealer_broker_Icap\n",
      "HIGH_TEMPERATURE\n",
      "Depressed_Classes\n",
      "Bajpe_airport_tabletop\n",
      "FASTx_TM\n",
      "Michel_GuÃ©rard\n",
      "Zheng_Enchong\n",
      "drivers_union_GdL\n",
      "Makes_Bearish\n",
      "Salvacion_Servano\n"
     ]
    }
   ],
   "source": [
    "# è¤‡åˆèªã®ä¾‹(word2vec)ã‚’è¡¨ç¤º\n",
    "import random\n",
    "\n",
    "\n",
    "print(\"\\nWord2Vecã®è¤‡åˆèªã®ä¾‹:\")\n",
    "compound_w2v_examples = [word for word in w2v_words if '_' in word]\n",
    "# ãƒ©ãƒ³ãƒ€ãƒ ã«å–ã£ã¦ããŸã„ãã¤ã‹ã‚’è¡¨ç¤º\n",
    "for i in random.sample(compound_w2v_examples, 100):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c01cf791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'look_up' in WordNet lemmas: True\n",
      "'look_up' in Word2Vec vocabulary: False\n",
      "'Look_up' in Word2Vec lemmas: False\n",
      "'LOOK_UP' in Word2Vec lemmas: True\n",
      "'look-up' in Word2Vec lemmas: False\n",
      "'lookUp' in Word2Vec lemmas: False\n",
      "'lookup' in Word2Vec lemmas: True\n"
     ]
    }
   ],
   "source": [
    "# look_upã‚’æ¢ã—ã¦ã¿ã‚‹\n",
    "# wordnet\n",
    "print(\"\\n'look_up' in WordNet lemmas:\", 'look_up' in raw_lemmas_set)\n",
    "\n",
    "# word2vec\n",
    "print(\"'look_up' in Word2Vec vocabulary:\", 'look_up' in w2v_words)\n",
    "\n",
    "# å½¢å¼ã‚’è‰²ã€…å¤‰ãˆã¦ã¿ã¦\n",
    "print(\"'Look_up' in Word2Vec lemmas:\", 'Look_up' in w2v_words)\n",
    "print(\"'LOOK_UP' in Word2Vec lemmas:\", 'LOOK_UP' in w2v_words)\n",
    "print(\"'look-up' in Word2Vec lemmas:\", 'look-up' in w2v_words)\n",
    "print(\"'lookUp' in Word2Vec lemmas:\", 'lookUp' in w2v_words)\n",
    "print(\"'lookup' in Word2Vec lemmas:\", 'lookup' in w2v_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d179abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of w2v_words_normalized: 2702146\n",
      "WordNetã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€™è£œç·æ•°: 294445\n",
      "\n",
      "==================================================\n",
      "ğŸ“š å…±é€šèªå½™ã®è¨ˆç®—çµæœ\n",
      "==================================================\n",
      "WordNet ç”Ÿãƒ¬ãƒæ•° (å…¥åŠ›): 147,478 å€‹\n",
      "WordNet å€™è£œç·æ•° (ç”Ÿæˆ): 294,445 å€‹\n",
      "--------------------------------------------------\n",
      "å…±é€šã™ã‚‹å˜èªã®æ•° (æœ€å¤§é™ä¸€è‡´): 78,600 å€‹\n",
      "==================================================\n",
      "\n",
      "å…±é€šã™ã‚‹å˜èªã®ä¾‹ (æœ€åˆã®10å€‹):\n",
      "1. 's\n",
      "2. 0\n",
      "3. 1\n",
      "4. 1st\n",
      "5. 1st_class\n",
      "6. 2\n",
      "7. 2d\n",
      "8. 2nd\n",
      "9. 3\n",
      "10. 3d\n"
     ]
    }
   ],
   "source": [
    "# å…±é€šéƒ¨åˆ†\n",
    "\n",
    "# --- 1. Word2Vecèªå½™ã®èª­ã¿è¾¼ã¿ã¨æ­£è¦åŒ– ---\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # Word2Vecã®èªå½™ã‚’å…¨ã¦å°æ–‡å­—ã«æ­£è¦åŒ–ã—ã¦ã‚»ãƒƒãƒˆã«æ ¼ç´\n",
    "        w2v_words_normalized = set(line.strip().lower() for line in f if line.strip())\n",
    "    \n",
    "    print(f\"length of w2v_words_normalized: {len(w2v_words_normalized)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ã‚¨ãƒ©ãƒ¼: Word2Vecãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}\")\n",
    "    w2v_words_normalized = set()\n",
    "\n",
    "# --- 2. WordNetãƒ¬ãƒã®å¤šè§’çš„å€™è£œç”Ÿæˆ ---\n",
    "\n",
    "wn_candidate_set = set()\n",
    "total_candidates = 0\n",
    "\n",
    "for raw_lemma in raw_lemmas_set:\n",
    "    # å…¨ã¦å°æ–‡å­—ã«å¤‰æ›ï¼ˆWord2Vecèªå½™ã‚‚å°æ–‡å­—åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ï¼‰\n",
    "    lower_lemma = raw_lemma.lower()\n",
    "    total_candidates += 1 # ç”Ÿã®ãƒ¬ãƒæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    \n",
    "    # å€™è£œãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–\n",
    "    candidates = set()\n",
    "    \n",
    "    # --- å€™è£œç”Ÿæˆ ---\n",
    "    \n",
    "    # 1. é€£çµå°æ–‡å­— (ä¾‹: look_up -> lookup, run-in -> runin)\n",
    "    #    ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã¨ãƒã‚¤ãƒ•ãƒ³ã‚’é™¤å»\n",
    "    candidates.add(lower_lemma.replace('_', '').replace('-', ''))\n",
    "    \n",
    "    # 2. ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šå°æ–‡å­— (ä¾‹: look_up -> look up, run-in -> run in)\n",
    "    candidates.add(lower_lemma.replace('_', ' ').replace('-', ' '))\n",
    "    \n",
    "    # 3. ç”Ÿå½¢å¼å°æ–‡å­— (ä¾‹: look_up -> look_up)\n",
    "    #    Word2VecãŒã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦æ®‹ã—ã¦ã„ã‚‹å ´åˆã«å¯¾å¿œ\n",
    "    candidates.add(lower_lemma)\n",
    "    \n",
    "    # 4. æ§‹æˆå˜èªã®å€‹åˆ¥æŠ½å‡º\n",
    "    #    '_', '-', ' ' ã®ã„ãšã‚Œã‹ã§åˆ†å‰²ã•ã‚Œã‚‹å˜èªãŒã‚ã‚Œã°ã€ãã‚Œã‚‰ã‚’å€‹åˆ¥ã«è¿½åŠ \n",
    "    if '_' in lower_lemma or '-' in lower_lemma:\n",
    "        parts = re.split(r'[_\\-\\s]+', lower_lemma)\n",
    "        for part in parts:\n",
    "            if part:\n",
    "                candidates.add(part)\n",
    "    \n",
    "    # å€™è£œã‚’å…¨ä½“ã®é›†åˆã«è¿½åŠ \n",
    "    wn_candidate_set.update(candidates)\n",
    "    \n",
    "print(f\"WordNetã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€™è£œç·æ•°: {len(wn_candidate_set)}\")\n",
    "\n",
    "\n",
    "# --- 3. å…±é€šéƒ¨åˆ†ã®è¨ˆç®— ---\n",
    "\n",
    "# å…±é€šé›†åˆ = (ç”Ÿæˆã•ã‚ŒãŸWordNetå€™è£œå…¨ã¦) âˆ© (æ­£è¦åŒ–ã•ã‚ŒãŸWord2Vecèªå½™)\n",
    "common_words = wn_candidate_set.intersection(w2v_words_normalized)\n",
    "\n",
    "# --- çµæœã®è¡¨ç¤º ---\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“š å…±é€šèªå½™ã®è¨ˆç®—çµæœ\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"WordNet ç”Ÿãƒ¬ãƒæ•° (å…¥åŠ›): {len(raw_lemmas_set):,} å€‹\")\n",
    "print(f\"WordNet å€™è£œç·æ•° (ç”Ÿæˆ): {len(wn_candidate_set):,} å€‹\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"å…±é€šã™ã‚‹å˜èªã®æ•° (æœ€å¤§é™ä¸€è‡´): {len(common_words):,} å€‹\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ä¾‹ã¨ã—ã¦ã€å–å¾—ã—ãŸå…±é€šèªå½™ã®ä¸€éƒ¨ã‚’è¡¨ç¤º\n",
    "if common_words:\n",
    "    print(\"\\nå…±é€šã™ã‚‹å˜èªã®ä¾‹ (æœ€åˆã®10å€‹):\")\n",
    "    # è¤‡åˆèªã ã£ãŸã‚‚ã®ã‚„ã€å›ºæœ‰åè©ã ã£ãŸã‚‚ã®ãŒå°æ–‡å­—ã§ä¸€è‡´ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹\n",
    "    for i, word in enumerate(sorted(list(common_words))[:10]):\n",
    "        print(f\"{i+1}. {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61182b35",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c38ade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vecèªå½™ãƒªã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ã—ã¾ã™...\n",
      "\n",
      "==================================================\n",
      "ğŸ“š Word2Vecèªå½™ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœï¼ˆè¤‡åˆèªãƒ»å¤§æ–‡å­—é™¤å»ã‚ã‚Šï¼‰\n",
      "==================================================\n",
      "| å‡¦ç†å‰ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªç”Ÿèªå½™ç·æ•°: 2,999,996 å€‹\n",
      "| ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªç·æ•°: 154,429 å€‹\n",
      "--------------------------------------------------\n",
      "| æœ€çµ‚çš„ã«é™¤å»ã•ã‚ŒãŸãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªç·æ•°: 2,845,567 å€‹\n",
      "--------------------------------------------------\n",
      "\n",
      "--- é™¤å»ç†ç”±ã®å†…è¨³ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ™ãƒ¼ã‚¹ï¼‰---\n",
      "| è¤‡åˆèª (ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ '_')                  : 2,070,977 å€‹\n",
      "| å›ºæœ‰åè©/å¤§æ–‡å­—ã‚’å«ã‚€                        : 746,281 å€‹\n",
      "| çŸ­ã™ãã‚‹å˜èª (3æ–‡å­—æœªæº€)                     : 1,201 å€‹\n",
      "| ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿                          : 0 å€‹\n",
      "| è¨˜å·ã‚„æ•°å­—ã‚’å«ã‚€                           : 27,108 å€‹\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®å˜èªã®ä¾‹ã‚’è¡¨ç¤º\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m sample_list \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mlist\u001b[39m(filtered_w2v_words), \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m20\u001b[39m, total_filtered_unique))\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªã®ä¾‹ (20å€‹):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28msorted\u001b[39m(sample_list)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# --- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¢æ•° ---\n",
    "import csv\n",
    "\n",
    "def clean_and_filter_w2v_word(word, exclusion_counts):\n",
    "    \"\"\"\n",
    "    Word2Vecã®å˜èªã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ä¸»è¦ãªå˜èªã¨ã—ã¦é©åˆ‡ã‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. è¤‡åˆèªï¼ˆã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ '_') ã®é™¤å¤–\n",
    "    if '_' in word:\n",
    "        exclusion_counts['Compound_Word'] += 1\n",
    "        return None\n",
    "\n",
    "    # 2. å›ºæœ‰åè©/å¤§æ–‡å­—ã®é™¤å¤– (å®Œå…¨ã«å°æ–‡å­—ã§ãªã„å˜èª)\n",
    "    if not word.islower():\n",
    "        exclusion_counts['Proper_Noun_or_Capitalized'] += 1\n",
    "        return None\n",
    "\n",
    "    # ã“ã“ã‹ã‚‰ã¯å°æ–‡å­—ã«æ­£è¦åŒ–ã•ã‚ŒãŸå˜èªã¨ã—ã¦æ‰±ã†\n",
    "    cleaned_word = word.lower() \n",
    "    \n",
    "    # 3. éå¸¸ã«çŸ­ã„å˜èªã®é™¤å» (3æ–‡å­—æœªæº€)\n",
    "    min_word_length = 3\n",
    "    if len(cleaned_word) < min_word_length:\n",
    "        exclusion_counts['Too_Short'] += 1\n",
    "        return None\n",
    "    \n",
    "    # 4. ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿ã®å˜èªã®é™¤å» (ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã¯æ—¢ã«é™¤å¤–ã•ã‚Œã¦ã„ã‚‹ãŒã€å¿µã®ãŸã‚)\n",
    "    if all(c == '_' for c in cleaned_word):\n",
    "        exclusion_counts['Underscore_Only'] += 1\n",
    "        return None\n",
    "        \n",
    "    # 5. è¨˜å·ãƒ»æ•°å­—ã®é™¤å» (ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¨ãƒã‚¤ãƒ•ãƒ³ä»¥å¤–)\n",
    "    # è¤‡åˆèªã¯æ—¢ã«é™¤å¤–ã•ã‚ŒãŸãŸã‚ã€ã“ã“ã§ã¯æ®‹ã£ãŸãƒã‚¤ãƒ•ãƒ³ã‚„ãƒã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯\n",
    "    if re.search(r'[^a-z\\-]', cleaned_word):\n",
    "        exclusion_counts['Symbol_or_Numeric'] += 1\n",
    "        return None\n",
    "        \n",
    "    return cleaned_word\n",
    "\n",
    "# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
    "raw_w2v_words_set = set(w2v_words) \n",
    "filtered_w2v_words = set()\n",
    "exclusion_counts = defaultdict(int)\n",
    "\n",
    "print(\"Word2Vecèªå½™ãƒªã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "\n",
    "for word in raw_w2v_words_set:\n",
    "    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨\n",
    "    cleaned_word = clean_and_filter_w2v_word(word, exclusion_counts)\n",
    "    \n",
    "    if cleaned_word:\n",
    "        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®å˜èªã¯å°æ–‡å­—åŒ–ã•ã‚Œã¦ã„ã‚‹\n",
    "        filtered_w2v_words.add(cleaned_word)\n",
    "\n",
    "# --- çµæœã®è¡¨ç¤º ---\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“š Word2Vecèªå½™ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœï¼ˆè¤‡åˆèªãƒ»å¤§æ–‡å­—é™¤å»ã‚ã‚Šï¼‰\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ç”Ÿã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªæ•°ã‚’è¨ˆç®—\n",
    "total_raw_unique = len(raw_w2v_words_set)\n",
    "# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªæ•°\n",
    "total_filtered_unique = len(filtered_w2v_words)\n",
    "# é™¤å»ã•ã‚ŒãŸå˜èªã®ç·æ•°\n",
    "total_removed = total_raw_unique - total_filtered_unique\n",
    "\n",
    "print(f\"| å‡¦ç†å‰ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªç”Ÿèªå½™ç·æ•°: {total_raw_unique:,} å€‹\")\n",
    "print(f\"| ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªç·æ•°: {total_filtered_unique:,} å€‹\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"| æœ€çµ‚çš„ã«é™¤å»ã•ã‚ŒãŸãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªç·æ•°: {total_removed:,} å€‹\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n--- é™¤å»ç†ç”±ã®å†…è¨³ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ™ãƒ¼ã‚¹ï¼‰---\")\n",
    "# é™¤å»ç†ç”±ã¨ãã®å˜èªæ•°ã®ãƒªã‚¹ãƒˆ\n",
    "removal_reasons = {\n",
    "    \"Compound_Word\": \"è¤‡åˆèª (ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ '_')\",\n",
    "    \"Proper_Noun_or_Capitalized\": \"å›ºæœ‰åè©/å¤§æ–‡å­—ã‚’å«ã‚€\",\n",
    "    \"Too_Short\": \"çŸ­ã™ãã‚‹å˜èª (3æ–‡å­—æœªæº€)\",\n",
    "    \"Underscore_Only\": \"ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿\",\n",
    "    \"Symbol_or_Numeric\": \"è¨˜å·ã‚„æ•°å­—ã‚’å«ã‚€\"\n",
    "}\n",
    "\n",
    "# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¢æ•°å†…ã§ã‚«ã‚¦ãƒ³ãƒˆã•ã‚ŒãŸæ•°ã‚’è¡¨ç¤º\n",
    "for key, description in removal_reasons.items():\n",
    "    count = exclusion_counts[key]\n",
    "    print(f\"| {description:<35}: {count:,} å€‹\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®å˜èªã®ä¾‹ã‚’è¡¨ç¤º\n",
    "sample_list = random.sample(list(filtered_w2v_words), min(20, total_filtered_unique))\n",
    "print(\"\\nãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªã®ä¾‹ (20å€‹):\")\n",
    "for i, word in enumerate(sorted(sample_list)):\n",
    "    print(f\"{i+1}. {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb8d158",
   "metadata": {},
   "source": [
    "# word2vecã®ãƒ•ã‚£ãƒ«ã‚¿ã¨ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3080eac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vecèªå½™ã¨WordNetãƒ¬ãƒã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...\n",
      "\n",
      "--- 1. W2Vã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒªã‚¹ãƒˆã®ä¿å­˜ ---\n",
      "âœ… ä¿å­˜æˆåŠŸ (w2v_cleaned): processed_data\\w2v_cleaned_indexed_words.csv\n",
      "   ä¿å­˜ã•ã‚ŒãŸå˜èªæ•°: 2,480,159 å€‹\n",
      "\n",
      "--- 2. WN/W2Vå…±é€šèªå½™ãƒªã‚¹ãƒˆã®ä¿å­˜ ---\n",
      "âœ… ä¿å­˜æˆåŠŸ (wn_w2v_common): processed_data\\wn_w2v_common_indexed_words.csv\n",
      "   ä¿å­˜ã•ã‚ŒãŸå˜èªæ•°: 77,936 å€‹\n",
      "\n",
      "--- å®Œäº† ---\n"
     ]
    }
   ],
   "source": [
    "# --- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¢æ•° (ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ç”¨) ---\n",
    "import csv\n",
    "# å˜ãªã‚‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã§ã¯è¤‡åˆèªã‚„å¤§æ–‡å­—ã¯é™¤å»ã›ãšã€å°æ–‡å­—åŒ–ã¨ãƒã‚¤ã‚ºé™¤å»ã®ã¿ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "def simple_clean_w2v_word(word):\n",
    "    \"\"\"ãƒã‚¤ã‚ºã¨çŸ­ã™ãã‚‹å˜èªã‚’é™¤å»ã—ã€å°æ–‡å­—ã«æ­£è¦åŒ–ã™ã‚‹ã€‚è¤‡åˆèªã¯ä¿æŒã€‚\"\"\"\n",
    "    \n",
    "    # 1. éå¸¸ã«çŸ­ã„å˜èªã®é™¤å» (3æ–‡å­—æœªæº€)\n",
    "    min_word_length = 3\n",
    "    if len(word) < min_word_length:\n",
    "        return None\n",
    "    \n",
    "    # 2. ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿ã®å˜èªã®é™¤å»\n",
    "    if all(c == '_' for c in word):\n",
    "        return None\n",
    "        \n",
    "    # 3. è¨˜å·ãƒ»æ•°å­—ã®é™¤å» (ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã€ãƒã‚¤ãƒ•ãƒ³ã€ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ä»¥å¤–)\n",
    "    # W2Vèªå½™ã¯è¤‡åˆèªã‚„ç•¥èªã¨ã—ã¦ã“ã‚Œã‚‰ã®è¨˜å·ã‚’å«ã‚€ã“ã¨ãŒå¤šã„ãŸã‚ã€æ…é‡ã«ãƒã‚§ãƒƒã‚¯\n",
    "    if re.search(r'[^a-zA-Z0-9_\\-]', word):\n",
    "        return None\n",
    "        \n",
    "    # 4. å°æ–‡å­—ã¸ã®æ­£è¦åŒ–\n",
    "    cleaned_word = word.lower()\n",
    "    \n",
    "    return cleaned_word\n",
    "\n",
    "# --- å…±é€šéƒ¨åˆ†è¨ˆç®—é–¢æ•° (WordNetã¨ã®æ¯”è¼ƒç”¨) ---\n",
    "\n",
    "def get_common_and_cleaned_words(raw_w2v_list, raw_wn_set):\n",
    "    \"\"\"\n",
    "    WordNetã¨Word2Vecã®èªå½™ã®å…±é€šéƒ¨åˆ†ã‚’è¨ˆç®—ã™ã‚‹ã€‚\n",
    "    WordNetã®ãƒ¬ãƒã‚’è¤‡æ•°ã®å½¢å¼ã«å¤‰æ›ã—ã€Word2Vecã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸèªå½™ã¨æ¯”è¼ƒã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Word2Vecèªå½™ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¨å°æ–‡å­—åŒ–\n",
    "    # W2Vèªå½™ã¯ã€å˜ãªã‚‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é©ç”¨ã—ã¦é›†åˆã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "    w2v_words_cleaned = set(filter(None, (simple_clean_w2v_word(w) for w in raw_w2v_list)))\n",
    "\n",
    "    # 2. WordNetãƒ¬ãƒã®å¤šè§’çš„å€™è£œç”Ÿæˆ (å°æ–‡å­—ã€é€£çµã€æ§‹æˆè¦ç´ ãªã©)\n",
    "    wn_candidate_set = set()\n",
    "    for raw_lemma in raw_wn_set:\n",
    "        lower_lemma = raw_lemma.lower()\n",
    "        \n",
    "        # å€™è£œç”Ÿæˆ\n",
    "        candidates = set()\n",
    "        \n",
    "        # a. é€£çµå°æ–‡å­— (ä¾‹: look_up -> lookup, run-in -> runin)\n",
    "        candidates.add(lower_lemma.replace('_', '').replace('-', ''))\n",
    "        \n",
    "        # b. ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šå°æ–‡å­— (ä¾‹: look_up -> look up, run-in -> run in)\n",
    "        candidates.add(lower_lemma.replace('_', ' ').replace('-', ' '))\n",
    "        \n",
    "        # c. ç”Ÿå½¢å¼å°æ–‡å­— (ä¾‹: look_up -> look_up)\n",
    "        candidates.add(lower_lemma)\n",
    "        \n",
    "        # d. æ§‹æˆå˜èªã®å€‹åˆ¥æŠ½å‡º (ä¾‹: big_deal -> big, deal)\n",
    "        if '_' in lower_lemma or '-' in lower_lemma:\n",
    "            parts = re.split(r'[_\\-\\s]+', lower_lemma)\n",
    "            for part in parts:\n",
    "                if part and simple_clean_w2v_word(part): # å˜èªãŒã‚¯ãƒªãƒ¼ãƒ³ã§ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "                    candidates.add(part)\n",
    "        \n",
    "        wn_candidate_set.update(candidates)\n",
    "        \n",
    "    # 3. å…±é€šéƒ¨åˆ†ã®è¨ˆç®—\n",
    "    # å…±é€šé›†åˆ = (ç”Ÿæˆã•ã‚ŒãŸWordNetå€™è£œå…¨ã¦) âˆ© (ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸWord2Vecèªå½™)\n",
    "    common_words = wn_candidate_set.intersection(w2v_words_cleaned)\n",
    "    \n",
    "    # å…±é€šèªå½™ã¨ã€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸW2Vèªå½™ã‚’ä¸¡æ–¹è¿”ã—ã¾ã™\n",
    "    return w2v_words_cleaned, common_words\n",
    "\n",
    "# --- ãƒ¡ã‚¤ãƒ³å‡¦ç†ã¨ä¿å­˜ ---\n",
    "\n",
    "def save_indexed_word_list(word_set, filename_prefix):\n",
    "    \"\"\"\n",
    "    å˜èªé›†åˆã‚’ã‚½ãƒ¼ãƒˆã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»˜ãã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    output_dir = 'processed_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    word_list_to_save = sorted(list(word_set))\n",
    "    output_path = os.path.join(output_dir, f'{filename_prefix}_indexed_words.csv')\n",
    "    \n",
    "    try:\n",
    "        with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['index', 'word'])\n",
    "            \n",
    "            for index, word in enumerate(word_list_to_save):\n",
    "                writer.writerow([index, word])\n",
    "                \n",
    "        print(f\"âœ… ä¿å­˜æˆåŠŸ ({filename_prefix}): {output_path}\")\n",
    "        print(f\"   ä¿å­˜ã•ã‚ŒãŸå˜èªæ•°: {len(word_list_to_save):,} å€‹\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ã‚¨ãƒ©ãƒ¼: {filename_prefix} ã®ä¿å­˜ä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- å®Ÿè¡Œ ---\n",
    "print(\"Word2Vecèªå½™ã¨WordNetãƒ¬ãƒã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "\n",
    "# 1. å…±é€šèªå½™ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸW2Vèªå½™ã‚’å–å¾—\n",
    "w2v_cleaned_set, common_words_set = get_common_and_cleaned_words(w2v_words, raw_lemmas_set)\n",
    "\n",
    "# 2. å˜ãªã‚‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸW2Vèªå½™ã®ä¿å­˜\n",
    "print(\"\\n--- 1. W2Vã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒªã‚¹ãƒˆã®ä¿å­˜ ---\")\n",
    "save_indexed_word_list(w2v_cleaned_set, 'w2v_cleaned')\n",
    "\n",
    "# 3. WordNetã¨ã®å…±é€šéƒ¨åˆ†ã®ä¿å­˜\n",
    "print(\"\\n--- 2. WN/W2Vå…±é€šèªå½™ãƒªã‚¹ãƒˆã®ä¿å­˜ ---\")\n",
    "save_indexed_word_list(common_words_set, 'wn_w2v_common')\n",
    "\n",
    "print(\"\\n--- å®Œäº† ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca6029e",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30e3a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å†å¸°å›æ•°ã®è¨­å®š\n",
    "RECURSION_LIMIT = 1000000\n",
    "import sys\n",
    "sys.setrecursionlimit(RECURSION_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be627bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸­å¿ƒçš„ãªSynset: animal.n.01 ('a living organism characterized by voluntary movement')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# WordNetãƒ‡ãƒ¼ã‚¿ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å ´åˆã¯å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "def extract_hypernym_tree(synset, depth=3, max_hyponyms=5, current_depth=0, node_list=None, edge_list=None):\n",
    "    \"\"\"\n",
    "    ç‰¹å®šã®Synsetã‚’èµ·ç‚¹ã«ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‹ãƒŸãƒ¼ï¼ˆä¸Šä½æ¦‚å¿µï¼‰ã¨ãƒ’ãƒãƒ‹ãƒŸãƒ¼ï¼ˆä¸‹ä½æ¦‚å¿µï¼‰ã®\n",
    "    éšå±¤æ§‹é€ ã‚’å†å¸°çš„ã«æŠ½å‡ºã—ã€ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚\n",
    "    \n",
    "    Args:\n",
    "        synset (Synset): ç¾åœ¨å‡¦ç†ä¸­ã®ã‚·ãƒãƒ‹ãƒ ã‚»ãƒƒãƒˆã€‚\n",
    "        depth (int): éšå±¤ã‚’ãŸã©ã‚‹æœ€å¤§æ·±åº¦ã€‚\n",
    "        max_hyponyms (int): å„éšå±¤ã§æŠ½å‡ºã™ã‚‹ä¸‹ä½æ¦‚å¿µã®æœ€å¤§æ•°ï¼ˆå¯è¦–åŒ–ã®è¤‡é›‘ã•ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    if node_list is None:\n",
    "        node_list = []\n",
    "    if edge_list is None:\n",
    "        edge_list = []\n",
    "    \n",
    "    if current_depth > depth:\n",
    "        return node_list, edge_list\n",
    "\n",
    "    # ãƒãƒ¼ãƒ‰åã®ç”Ÿæˆ: Synsetå (å®šç¾©ã®æœ€åˆã®ãƒ¬ãƒ)\n",
    "    node_id = synset.name()\n",
    "    node_label = synset.lemmas()[0].name().replace('_', ' ')\n",
    "    \n",
    "    if node_id not in [n['id'] for n in node_list]:\n",
    "        node_list.append({\n",
    "            'id': node_id,\n",
    "            'label': node_label,\n",
    "            'depth': current_depth\n",
    "        })\n",
    "\n",
    "    # --- 1. ä¸Šä½æ¦‚å¿µï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‹ãƒŸãƒ¼ï¼‰ã‚’æŠ½å‡º ---\n",
    "    # é€šå¸¸ã€å˜ä¸€ãƒ‘ã‚¹ï¼ˆã¾ãŸã¯å°‘ãªã„ãƒ‘ã‚¹ï¼‰ã®ä¸Šä½æ¦‚å¿µã‚’ãŸã©ã‚‹\n",
    "    hypernyms = synset.hypernyms()\n",
    "    for hypernym in hypernyms:\n",
    "        hypernym_id = hypernym.name()\n",
    "        # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ  (ä¸‹ä½ -> ä¸Šä½)\n",
    "        if (node_id, hypernym_id) not in edge_list:\n",
    "            edge_list.append({'source': node_id, 'target': hypernym_id, 'type': 'is-a'})\n",
    "        \n",
    "        # ä¸Šä½æ¦‚å¿µã‚’å†å¸°çš„ã«å‡¦ç† (æ·±ã•åˆ¶é™å†…ã§)\n",
    "        if current_depth > -depth: # éšå±¤ã‚’ä¸Šã«é¡ã‚‹ãŸã‚ã€ãƒã‚¤ãƒŠã‚¹æ–¹å‘ã«å‡¦ç†\n",
    "            extract_hypernym_tree(hypernym, depth, max_hyponyms, current_depth - 1, node_list, edge_list)\n",
    "\n",
    "    # --- 2. ä¸‹ä½æ¦‚å¿µï¼ˆãƒ’ãƒãƒ‹ãƒŸãƒ¼ï¼‰ã‚’æŠ½å‡º ---\n",
    "    # ä¸‹ä½æ¦‚å¿µã¯æ•°ãŒå¤šã™ãã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€åˆ¶é™ã‚’è¨­ã‘ã‚‹\n",
    "    hyponyms = synset.hyponyms()\n",
    "    \n",
    "    for i, hyponym in enumerate(hyponyms):\n",
    "        if i >= max_hyponyms:\n",
    "            # max_hyponyms ã‚’è¶…ãˆãŸã‚‰çœç•¥ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ \n",
    "            if current_depth < depth and node_id + \"_omission\" not in [n['id'] for n in node_list]:\n",
    "                omission_id = node_id + \"_omission\"\n",
    "                node_list.append({'id': omission_id, 'label': '...', 'depth': current_depth + 1})\n",
    "                edge_list.append({'source': node_id, 'target': omission_id, 'type': 'omitted'})\n",
    "            break\n",
    "            \n",
    "        hyponym_id = hyponym.name()\n",
    "        # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ  (ä¸Šä½ -> ä¸‹ä½)\n",
    "        if (node_id, hyponym_id) not in edge_list:\n",
    "            edge_list.append({'source': hyponym_id, 'target': node_id, 'type': 'is-a'}) # å‘ãã‚’çµ±ä¸€\n",
    "            \n",
    "        # ä¸‹ä½æ¦‚å¿µã‚’å†å¸°çš„ã«å‡¦ç†\n",
    "        extract_hypernym_tree(hyponym, depth, max_hyponyms, current_depth + 1, node_list, edge_list)\n",
    "        \n",
    "    return node_list, edge_list\n",
    "\n",
    "# --- å®Ÿè¡Œ ---\n",
    "\n",
    "# ä¸­å¿ƒçš„ãªå˜èªï¼ˆã‚·ãƒãƒ‹ãƒ ã‚»ãƒƒãƒˆï¼‰ã‚’è¨­å®š\n",
    "# ä¾‹: 'dog' ã®åè©ã‚·ãƒãƒ‹ãƒ ã®æœ€åˆã®ã‚‚ã® ('dog.n.01')\n",
    "center_word = 'animal'\n",
    "synsets = wn.synsets(center_word, pos=wn.NOUN)\n",
    "\n",
    "if synsets:\n",
    "    root_synset = synsets[0] \n",
    "    print(f\"ä¸­å¿ƒçš„ãªSynset: {root_synset.name()} ('{root_synset.definition()}')\")\n",
    "\n",
    "    # éšå±¤ã®æ·±ã•ã‚’è¨­å®š\n",
    "    DEPTH_LIMIT = 2\n",
    "    MAX_HYPONYMS = 4\n",
    "    \n",
    "    # éšå±¤æ§‹é€ ã‚’æŠ½å‡º\n",
    "    nodes, edges = extract_hypernym_tree(\n",
    "        root_synset, \n",
    "        depth=DEPTH_LIMIT, \n",
    "        max_hyponyms=MAX_HYPONYMS, \n",
    "        current_depth=0\n",
    "    )\n",
    "\n",
    "    # --- çµæœã®è¡¨ç¤º ---\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"ğŸŒ² æŠ½å‡ºã•ã‚ŒãŸWordNetéšå±¤æ§‹é€  ({center_word})\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(f\"ãƒãƒ¼ãƒ‰ç·æ•°: {len(nodes)}\")\n",
    "    print(f\"ã‚¨ãƒƒã‚¸ç·æ•°: {len(edges)}\")\n",
    "    \n",
    "    print(\"\\n--- ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã®ä¾‹ ---\")\n",
    "    for node in nodes[:10]:\n",
    "        print(f\"ID: {node['id']}, Label: {node['label']}, Depth: {node['depth']}\")\n",
    "    \n",
    "    print(\"\\n--- ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆã®ä¾‹ ---\")\n",
    "    for edge in edges[:10]:\n",
    "        # source -> target ãŒ (ä¸‹ä½æ¦‚å¿µ -> ä¸Šä½æ¦‚å¿µ) ã®é–¢ä¿‚ã‚’è¡¨ã—ã¾ã™\n",
    "        print(f\"Source: {edge['source']} -> Target: {edge['target']}, Type: {edge['type']}\")\n",
    "else:\n",
    "    print(f\"'{center_word}' ã«å¯¾å¿œã™ã‚‹åè©ã®SynsetãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
