{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1912a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import inv, det\n",
    "import warnings\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from typing import Dict\n",
    "# --- 1. åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š ---\n",
    "\n",
    "def estimate_single_gaussian_params(X_data: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    å˜ä¸€ã®é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã€å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã¨å…±åˆ†æ•£è¡Œåˆ—ã‚’æ¨å®šã™ã‚‹ã€‚\n",
    "    ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒæ¬¡å…ƒæ•°ã‚ˆã‚Šå°‘ãªã„å ´åˆã€Ledoit-Wolfåç¸®æ³•ã‚’ç”¨ã„ã¦å…±åˆ†æ•£è¡Œåˆ—ã‚’é ‘å¥ã«æ¨å®šã™ã‚‹ã€‚\n",
    "\n",
    "    Args:\n",
    "        X_data (np.ndarray): å˜ä¸€ã®ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰ã«å±ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ (N, D)ã€‚\n",
    "\n",
    "    Returns:\n",
    "        Dict: {'mu': å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«, 'Sigma': å…±åˆ†æ•£è¡Œåˆ—}\n",
    "    \"\"\"\n",
    "    N, D = X_data.shape # ã‚µãƒ³ãƒ—ãƒ«æ•° N, æ¬¡å…ƒæ•° D\n",
    "\n",
    "    if N == 0:\n",
    "        raise ValueError(\"Input data array must not be empty.\")\n",
    "    \n",
    "    # 1. å¹³å‡ãƒ™ã‚¯ãƒˆãƒ« (mu) ã®æ¨å®š\n",
    "    mu = np.mean(X_data, axis=0)\n",
    "\n",
    "    # 2. å…±åˆ†æ•£è¡Œåˆ— (Sigma) ã®æ¨å®š\n",
    "    if N == 1:\n",
    "        # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒ1ã®å ´åˆã€å…±åˆ†æ•£ã¯ã‚¼ãƒ­\n",
    "        warnings.warn(\"N=1. Covariance matrix is set to zero (plus regularization).\")\n",
    "        Sigma = np.zeros((D, D))\n",
    "        \n",
    "    elif N < D + 1:\n",
    "        # N < D+1 ã®å ´åˆ: ç‰¹ç•°è¡Œåˆ—ã«ãªã‚‹ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚ã€Ledoit-Wolfåç¸®æ¨å®šã‚’ä½¿ç”¨\n",
    "        warnings.warn(f\"N={N} < D+1={D+1}. Using Ledoit-Wolf shrinkage for robust covariance estimation.\")\n",
    "        \n",
    "        # Ledoit-Wolfåç¸®æ¨å®šå™¨ã‚’åˆæœŸåŒ–ãƒ»å­¦ç¿’\n",
    "        lw = LedoitWolf()\n",
    "        lw.fit(X_data)\n",
    "        Sigma = lw.covariance_\n",
    "        \n",
    "    else:\n",
    "        # N >= D+1 ã®å ´åˆ: æ¨™æº–çš„ãªæœ€å°¤æ¨å®šã‚’ä½¿ç”¨\n",
    "        Sigma = np.cov(X_data, rowvar=False)\n",
    "\n",
    "    # 3. æœ€çµ‚çš„ãªæ­£å‰‡åŒ–ãƒã‚§ãƒƒã‚¯\n",
    "    # åç¸®æ¨å®šã‚’è¡Œã£ã¦ã‚‚ãªãŠæ¡ä»¶æ•°ãŒæ‚ªã„å ´åˆã«ã€å¾®å°ãªå€¤ã‚’åŠ ç®—\n",
    "    if np.linalg.cond(Sigma) > 1e15: # ã‚ˆã‚Šå³ã—ã„æ¡ä»¶æ•°ã§ãƒã‚§ãƒƒã‚¯\n",
    "        warnings.warn(\"Covariance matrix highly ill-conditioned. Applying final small regularization.\")\n",
    "        Sigma += np.eye(D) * 1e-6\n",
    "        \n",
    "    return {'mu': mu, 'Sigma': Sigma}\n",
    "\n",
    "# --- 2. é¡ä¼¼åº¦æ¸¬å®š (éé¡ä¼¼åº¦) ---\n",
    "\n",
    "def kl_divergence_gaussian(mu1, Sigma1, mu2, Sigma2) -> float:\n",
    "    \"\"\"\n",
    "    å¤šå¤‰é‡ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ N1 ã‹ã‚‰ N2 ã¸ã®KLæƒ…å ±é‡ D_KL(N1 || N2) ã‚’è¨ˆç®—ã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    D = mu1.shape[0]\n",
    "\n",
    "    try:\n",
    "        Sigma2_inv = inv(Sigma2)\n",
    "    except np.linalg.LinAlgError:\n",
    "        warnings.warn(\"Sigma2 is singular. KL divergence is undefined.\")\n",
    "        return np.nan\n",
    "\n",
    "    # 1. ãƒ­ã‚°ãƒ‡ã‚¿ãƒ¼ãƒŸãƒŠãƒ³ãƒˆé …\n",
    "    log_det_term = np.log(det(Sigma2) / det(Sigma1))\n",
    "\n",
    "    # 2. ãƒˆãƒ¬ãƒ¼ã‚¹é …\n",
    "    trace_term = np.trace(Sigma2_inv @ Sigma1)\n",
    "\n",
    "    # 3. ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢é … (å¹³å‡ã®é•ã„)\n",
    "    diff_mu = mu2 - mu1\n",
    "    mahalanobis_term = diff_mu.T @ Sigma2_inv @ diff_mu\n",
    "\n",
    "    kl_div = 0.5 * (log_det_term + trace_term + mahalanobis_term - D)\n",
    "\n",
    "    return kl_div\n",
    "\n",
    "# --- 3. é¡ä¼¼åº¦æ¸¬å®š (é‡ãªã‚Š) ---\n",
    "\n",
    "def bhattacharyya_coefficient_gaussian(mu1, Sigma1, mu2, Sigma2) -> float:\n",
    "    \"\"\"\n",
    "    å¤šå¤‰é‡ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã®ãƒã‚¿ãƒãƒ£ãƒªãƒ¤ä¿‚æ•° BC ã‚’è¨ˆç®—ã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    D = mu1.shape[0]\n",
    "    \n",
    "    # å…±åˆ†æ•£è¡Œåˆ—ã®å¹³å‡ Sigma = 0.5 * (Sigma1 + Sigma2)\n",
    "    Sigma = 0.5 * (Sigma1 + Sigma2)\n",
    "\n",
    "    try:\n",
    "        Sigma_inv = inv(Sigma)\n",
    "    except np.linalg.LinAlgError:\n",
    "        warnings.warn(\"Sigma (mean covariance) is singular. Bhattacharyya Coefficient is undefined.\")\n",
    "        return np.nan\n",
    "\n",
    "    # 1. ãƒã‚¿ãƒãƒ£ãƒªãƒ¤è·é›¢ã®å¹³å‡é …ï¼ˆãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã®å¤‰å½¢ï¼‰\n",
    "    diff_mu = mu1 - mu2\n",
    "    db_mu_term = 0.125 * diff_mu.T @ Sigma_inv @ diff_mu\n",
    "\n",
    "    # 2. ãƒã‚¿ãƒãƒ£ãƒªãƒ¤è·é›¢ã®å…±åˆ†æ•£é …\n",
    "    db_cov_term = 0.5 * np.log(det(Sigma) / np.sqrt(det(Sigma1) * det(Sigma2)))\n",
    "\n",
    "    # ãƒã‚¿ãƒãƒ£ãƒªãƒ¤è·é›¢\n",
    "    db_distance = db_mu_term + db_cov_term\n",
    "    \n",
    "    # ãƒã‚¿ãƒãƒ£ãƒªãƒ¤ä¿‚æ•° = exp(-è·é›¢)\n",
    "    bc = np.exp(-db_distance)\n",
    "\n",
    "    return bc\n",
    "\n",
    "# --- 4. ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ (å‚è€ƒ) ---\n",
    "\n",
    "def mahalanobis_distance(mu1, mu2, Sigma_pooled) -> float:\n",
    "    \"\"\"\n",
    "    ãƒ—ãƒ¼ãƒ«ã•ã‚ŒãŸå…±åˆ†æ•£è¡Œåˆ— Sigma_pooled ã‚’ç”¨ã„ã¦ã€2ã¤ã®å¹³å‡é–“ã®ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã‚’è¨ˆç®—ã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Sigma_inv = inv(Sigma_pooled)\n",
    "    except np.linalg.LinAlgError:\n",
    "        warnings.warn(\"Sigma_pooled is singular. Mahalanobis distance is undefined.\")\n",
    "        return np.nan\n",
    "    \n",
    "    diff_mu = mu1 - mu2\n",
    "    # è·é›¢ã®2ä¹—\n",
    "    dist_sq = diff_mu.T @ Sigma_inv @ diff_mu\n",
    "    \n",
    "    return np.sqrt(dist_sq)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e4c1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ— 1: åˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¨å®šçµæœ\n",
      "ã‚¯ãƒ©ã‚¹ã‚¿ A: mu_norm=0.24, det(Sigma)=4.10e+02\n",
      "ã‚¯ãƒ©ã‚¹ã‚¿ B: mu_norm=4.90, det(Sigma)=5.92e-01\n",
      "ã‚¯ãƒ©ã‚¹ã‚¿ C: mu_norm=0.48, det(Sigma)=2.31e+01\n",
      "\n",
      "## ğŸ“ˆ ã‚¹ãƒ†ãƒƒãƒ— 2: ã‚¯ãƒ©ã‚¹ã‚¿é–“é¡ä¼¼åº¦ã®è¨ˆç®—\n",
      "--- A (åˆ†æ•£å¤§) vs B (åˆ†æ•£å°ã€ä½ç½®é ã„) ---\n",
      "KLæƒ…å ±é‡ D_KL(A || B) (éé¡ä¼¼åº¦): 16.2178\n",
      "KLæƒ…å ±é‡ D_KL(B || A) (éé¡ä¼¼åº¦): 7.8807 (éå¯¾ç§°æ€§ã®ä¾‹)\n",
      "ãƒã‚¿ãƒãƒ£ãƒªãƒ¤ä¿‚æ•° BC(A, B) (é‡ãªã‚Š): 0.0884 (1ã«è¿‘ã„ã»ã©é¡ä¼¼)\n",
      "\n",
      "--- A (çƒçŠ¶) vs C (ç´°é•·ã„ã€ä½ç½®è¿‘ã„) ---\n",
      "KLæƒ…å ±é‡ D_KL(A || C): 2.7139\n",
      "KLæƒ…å ±é‡ D_KL(C || A): 1.5463\n",
      "ãƒã‚¿ãƒãƒ£ãƒªãƒ¤ä¿‚æ•° BC(A, C): 0.6450\n"
     ]
    }
   ],
   "source": [
    "# --- ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ ---\n",
    "np.random.seed(42)\n",
    "\n",
    "# é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ (D=10)\n",
    "D = 10\n",
    "N_A = 100\n",
    "N_B = 120\n",
    "N_C = 80\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
    "mu_A = np.zeros(D)\n",
    "Sigma_A = np.eye(D) * 2\n",
    "\n",
    "mu_B = np.ones(D) * 1.5 # å°‘ã—é›¢ã‚ŒãŸä½ç½®\n",
    "Sigma_B = np.eye(D) * 1 # åˆ†æ•£ãŒå°ã•ã„ï¼ˆå¯†ï¼‰\n",
    "\n",
    "mu_C = np.zeros(D) # Aã¨åŒã˜ä½ç½®ã ãŒå½¢çŠ¶ãŒç•°ãªã‚‹\n",
    "Sigma_C = np.diag(np.linspace(0.5, 3.0, D)) # éå¸¸ã«ç´°é•·ã„å½¢çŠ¶\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "X_A = np.random.multivariate_normal(mu_A, Sigma_A, N_A)\n",
    "X_B = np.random.multivariate_normal(mu_B, Sigma_B, N_B)\n",
    "X_C = np.random.multivariate_normal(mu_C, Sigma_C, N_C)\n",
    "\n",
    "# å…ƒãƒ‡ãƒ¼ã‚¿ X ã‚’ä½œæˆ (N=300, D=10)\n",
    "X = np.vstack([X_A, X_B, X_C])\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹ã‚¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¾æ›¸\n",
    "cluster_indices = {\n",
    "    'A': list(range(0, N_A)),\n",
    "    'B': list(range(N_A, N_A + N_B)),\n",
    "    'C': list(range(N_A + N_B, N_A + N_B + N_C))\n",
    "}\n",
    "\n",
    "## ã‚¹ãƒ†ãƒƒãƒ— 1: åˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¨å®š\n",
    "params = estimate_gaussian_params(X, cluster_indices)\n",
    "\n",
    "print(\"## ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ— 1: åˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¨å®šçµæœ\")\n",
    "print(f\"ã‚¯ãƒ©ã‚¹ã‚¿ A: mu_norm={np.linalg.norm(params['A']['mu']):.2f}, det(Sigma)={det(params['A']['Sigma']):.2e}\")\n",
    "print(f\"ã‚¯ãƒ©ã‚¹ã‚¿ B: mu_norm={np.linalg.norm(params['B']['mu']):.2f}, det(Sigma)={det(params['B']['Sigma']):.2e}\")\n",
    "print(f\"ã‚¯ãƒ©ã‚¹ã‚¿ C: mu_norm={np.linalg.norm(params['C']['mu']):.2f}, det(Sigma)={det(params['C']['Sigma']):.2e}\")\n",
    "\n",
    "## ã‚¹ãƒ†ãƒƒãƒ— 2: é¡ä¼¼åº¦æ¸¬å®šã®å®Ÿè¡Œ (Aã¨Bã€Aã¨Cã‚’æ¯”è¼ƒ)\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡º\n",
    "mu_A, Sigma_A = params['A']['mu'], params['A']['Sigma']\n",
    "mu_B, Sigma_B = params['B']['mu'], params['B']['Sigma']\n",
    "mu_C, Sigma_C = params['C']['mu'], params['C']['Sigma']\n",
    "\n",
    "print(\"\\n## ğŸ“ˆ ã‚¹ãƒ†ãƒƒãƒ— 2: ã‚¯ãƒ©ã‚¹ã‚¿é–“é¡ä¼¼åº¦ã®è¨ˆç®—\")\n",
    "print(\"--- A (åˆ†æ•£å¤§) vs B (åˆ†æ•£å°ã€ä½ç½®é ã„) ---\")\n",
    "kl_AB = kl_divergence_gaussian(mu_A, Sigma_A, mu_B, Sigma_B)\n",
    "kl_BA = kl_divergence_gaussian(mu_B, Sigma_B, mu_A, Sigma_A)\n",
    "bc_AB = bhattacharyya_coefficient_gaussian(mu_A, Sigma_A, mu_B, Sigma_B)\n",
    "\n",
    "print(f\"KLæƒ…å ±é‡ D_KL(A || B) (éé¡ä¼¼åº¦): {kl_AB:.4f}\")\n",
    "print(f\"KLæƒ…å ±é‡ D_KL(B || A) (éé¡ä¼¼åº¦): {kl_BA:.4f} (éå¯¾ç§°æ€§ã®ä¾‹)\")\n",
    "print(f\"ãƒã‚¿ãƒãƒ£ãƒªãƒ¤ä¿‚æ•° BC(A, B) (é‡ãªã‚Š): {bc_AB:.4f} (1ã«è¿‘ã„ã»ã©é¡ä¼¼)\")\n",
    "\n",
    "print(\"\\n--- A (çƒçŠ¶) vs C (ç´°é•·ã„ã€ä½ç½®è¿‘ã„) ---\")\n",
    "kl_AC = kl_divergence_gaussian(mu_A, Sigma_A, mu_C, Sigma_C)\n",
    "kl_CA = kl_divergence_gaussian(mu_C, Sigma_C, mu_A, Sigma_A)\n",
    "bc_AC = bhattacharyya_coefficient_gaussian(mu_A, Sigma_A, mu_C, Sigma_C)\n",
    "\n",
    "print(f\"KLæƒ…å ±é‡ D_KL(A || C): {kl_AC:.4f}\")\n",
    "print(f\"KLæƒ…å ±é‡ D_KL(C || A): {kl_CA:.4f}\")\n",
    "print(f\"ãƒã‚¿ãƒãƒ£ãƒªãƒ¤ä¿‚æ•° BC(A, C): {bc_AC:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7504e",
   "metadata": {},
   "source": [
    "# metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b0287ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cluster_similarity(vecs_a, vecs_b, mode='medoid'):\n",
    "    \"\"\"\n",
    "    2ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«é›†åˆé–“ã®é¡ä¼¼åº¦/è·é›¢ã‚’è¨ˆç®—ã™ã‚‹\n",
    "    \n",
    "    Parameters:\n",
    "    vecs_a, vecs_b: np.array (n_samples, n_features)\n",
    "    mode: 'medoid' (ä»£è¡¨ç‚¹åŒå£«), 'average' (å…¨ä½“ã®å¹³å‡)\n",
    "    \n",
    "    Returns:\n",
    "    dict: é¡ä¼¼åº¦ã¨è·é›¢\n",
    "    \"\"\"\n",
    "    \n",
    "    if mode == 'medoid':\n",
    "        # å„é›†åˆå†…ã§ã€Œä»–ã®ç‚¹ã¨ã®è·é›¢ã®ç·å’ŒãŒæœ€å°ã€ãªç‚¹ã‚’ãƒ¡ãƒ‰ã‚¤ãƒ‰ã¨ã™ã‚‹\n",
    "        # ç°¡æ˜“çš„ã«ä¸­å¿ƒç‚¹ã«æœ€ã‚‚è¿‘ã„å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ\n",
    "        def get_medoid(vecs):\n",
    "            centroid = np.mean(vecs, axis=0).reshape(1, -1)\n",
    "            # é‡å¿ƒã«æœ€ã‚‚è¿‘ã„ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¢ã™\n",
    "            sim = cosine_similarity(vecs, centroid)\n",
    "            return vecs[np.argmax(sim)].reshape(1, -1)\n",
    "        \n",
    "        rep_a = get_medoid(vecs_a)\n",
    "        rep_b = get_medoid(vecs_b)\n",
    "    else:\n",
    "        # å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ï¼‰ã‚’ä½¿ç”¨\n",
    "        rep_a = np.mean(vecs_a, axis=0).reshape(1, -1)\n",
    "        rep_b = np.mean(vecs_b, axis=0).reshape(1, -1)\n",
    "\n",
    "    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ç®—å‡º\n",
    "    similarity = cosine_similarity(rep_a, rep_b)[0][0]\n",
    "    \n",
    "    # è·é›¢ã«å¤‰æ› (0.0: åŒä¸€ ï½ 2.0: æ­£åå¯¾)\n",
    "    distance = 1 - similarity\n",
    "    \n",
    "    return {\n",
    "        \"similarity\": similarity,\n",
    "        \"distance\": distance\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca4ab9b",
   "metadata": {},
   "source": [
    "# å®Ÿãƒ‡ãƒ¼ã‚¿\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7710e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«\n",
    "vectors = np.load(\"../../d3-app/data/vector.npy\")\n",
    "# point_to_cluster_map\n",
    "point_to_cluster_map = np.load(\"../../d3-app/data/point_cluster_map.npy\")\n",
    "\n",
    "# unique clusters\n",
    "unique_clusters = np.unique(point_to_cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e4ef08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a21937ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_cluster_similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     vecs_b \u001b[38;5;241m=\u001b[39m vectors[point_to_cluster_map \u001b[38;5;241m==\u001b[39m cluster_b]\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# é¡ä¼¼åº¦è¨ˆç®—\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_cluster_similarity\u001b[49m(vecs_a, vecs_b, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m     similarity_results[(cluster_a, cluster_b)] \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# save\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'calculate_cluster_similarity' is not defined"
     ]
    }
   ],
   "source": [
    "# çµ„ã¿åˆã‚ã›ã”ã¨ã«é¡ä¼¼åº¦ã‚’è¨ˆç®—ã€è¾æ›¸ã«ä¿å­˜\n",
    "\n",
    "from itertools import combinations\n",
    "similarity_results = {}\n",
    "for cluster_a, cluster_b in combinations(unique_clusters, 2):\n",
    "    # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º\n",
    "    vecs_a = vectors[point_to_cluster_map == cluster_a]\n",
    "    vecs_b = vectors[point_to_cluster_map == cluster_b]\n",
    "    \n",
    "    # é¡ä¼¼åº¦è¨ˆç®—\n",
    "    result = calculate_cluster_similarity(vecs_a, vecs_b, mode='medoid')\n",
    "    similarity_results[(cluster_a, cluster_b)] = result[\"distance\"]\n",
    "\n",
    "# save\n",
    "np.save(\"src/experiments/21_cluster_similarity/cluster_similarity_distances.npy\", similarity_results)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
